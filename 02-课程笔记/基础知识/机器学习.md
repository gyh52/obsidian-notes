## 概率近似正确学习模型
![[Pasted image 20230314100232.png|350]]
f(x)相当于假设，y是真实值

## 概念
### 泛化
处理新数据的能力
（相当于关注ε的值，ε大于一定值，说明无法用机器学习解决）

### 归纳偏好（归置）
任何一个有效的机器学习算法都有其偏好
（偏好与问题匹配）

### 奥卡姆剃刀
若非必要勿增实体
（各种理论都能解释的情况下最简单的是最好的）

### NFL定理
![[Pasted image 20230314103137.png|500]]
(没有绝对好的算法，要根据具体问题具体分析)

### 泛化误差与经验误差
![[Pasted image 20230314103935.png|400]]
训练误差不是越小越好，没有完美的解决方案达到刚好拟合

### 过拟合与欠拟合
![[Pasted image 20230314104237.png|500]]
## 模型选择
![[Pasted image 20230314104629.png|400]]
### 评估方法
![[Pasted image 20230314104729.png|300]]

#### 超参数与参数
超参数：模型中人工设定
参数：学习后确定
调参：先产生模型，基于评估方法选择

#### 验证集
整个数据分为训练集和测试集，训练集选取一部分作为验证集，剩下的训练集训练模型，验证集调参数，选定参数确定模型后才使用测试集。
![[Pasted image 20230315090823.png|600]]

### 性能度量
衡量模型泛化能力的评价标准
（没有绝对好的模型，要取决于算法数据任务需求）

#### 均方误差
![[Pasted image 20230315091444.png|400]]
可能有常数二分之一，不影响最终结果，但方便计算中抵消。

#### 错误率和精度
![[Pasted image 20230315091737.png|500]]

#### 查准率（精度）和查全率
![[Pasted image 20230315091944.png|400]]

#### F1度量
P和R的调和平均值
![[Pasted image 20230315092208.png|400]]
加权重
![[Pasted image 20230315092324.png|500]]

### 比较检验
是概率近似正确不能直接比较，所以使用统计假设检验
两个学习器比较：
交叉验证t检验（交叉验证取差值，差值平均）（统计显著性）
McNemar检验

## 线性模型
![[Pasted image 20230315093117.png|500]]
**线性回归目的是找b。**
参数x是个值，如果参数不是连续属性，需要将离散属性**转化**为连续。
转化：若有序，按照序设定，若无序，将其变成k维向量。

### 线性回归过程
均方误差式中f(x)代入线性回归方程，对此式进行最小二乘估计
![[Pasted image 20230315093847.png|500]]

### 最小二乘参数估计
求导（找变化率）再令导数为0（变化率为0），目的是为了找极值点
![[Pasted image 20230315094030.png|500]]

## 多元线性回归
### 模型
![[Pasted image 20230315095006.png|500]]

令b为b * 1，分别表示出x和w，原式相当于x和w两个向量相乘，b包含在w中了。
![[Pasted image 20230315095050.png|350]]

### 最小二乘法过程
Xw相当于f(x)，b包含在w中了，跟上面公式一样求均方误差，求导，导数为0，得最终值。（得到的解非常复杂，需要正则化）
![[Pasted image 20230315095453.png|400]]
若满秩或正定有唯一解，否则有多解。

## 广义线性回归模型
### 对数线性回归
y的导数为线性
![[Pasted image 20230330092820.png|600]]
### 模型
![[Pasted image 20230330093052.png|300]]
### 对率回归
#### 过程
1. 分类任务想要输出0,1
![[Pasted image 20230330093745.png|500]]
2. 但理想情况下的函数不可微可导
![[Pasted image 20230330093758.png|550]]
3. 就找替代函数，对数机率函数（直译为逻辑函数，但与逻辑无关）
![[Pasted image 20230330093830.png|500]]
4. 把z代入，变成联系函数
![[Pasted image 20230330094209.png|550]]
几率=正例的可能性/负例的可能性，大于1为正例，小于1为负例。
![[Pasted image 20230330094231.png|200]]
5. 几率取对数得到，对数几率
![[Pasted image 20230330094551.png|400]]

#### 求解
![[Pasted image 20230330095320.png|500]]
正例：分子越大，负例：分母越大

不能用最小二乘法了，只有凸函数能用梯度为零求解，因为其他函数梯度为零的点不一定是极值点。

### 极大似然法
思想如下：通常取对数，因为相乘的概率数可能很小，浮点数相乘误差大，取对数变成相加。
![[Pasted image 20230330095556.png|300]]
代入y和f(x)，取对数，加上求最大值：
![[Pasted image 20230330100129.png|300]]
本算法用于分类，y=0或1，得到通项式：
![[Pasted image 20230330100248.png|300]]
最终得到：
![[Pasted image 20230330100517.png|500]]

化简得：
![[Pasted image 20230330100820.png|250]]
即：是高阶连续可导函数，可以用梯度下降了
![[Pasted image 20230330100829.png|200]]

## 类别不平衡
### 定义
![[Pasted image 20230330101710.png|350]]
正常情况下两类按1/2分，但类别不平衡不能这样分了。
![[Pasted image 20230330101824.png|600]]
![[Pasted image 20230330101952.png|300]]
### 解决方法
![[Pasted image 20230330102059.png|250]]
#### 过采样
小类增加，跟大类一样多。
复制的话很可能过拟合。
如SMOTE，插值，取样本中值
#### 欠采样
大类增加至小类一样多
不能随机丢。
#### 阈值移动
把0.5移动，如指示向量机

