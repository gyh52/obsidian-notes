# 概述
## 基本概念
state S
action A
reward R：大小自己定

policy π：博弈中一般为随机抽样
	![[Pasted image 20230220143906.png|250]]
state transition 状态转移
	从旧状态转移到新状态
	有随机性
	用p函数表示：
	![[Pasted image 20230220144351.png|300]]
		在当前智能体的动作和状态下，新状态的概率

## 过程
![[Pasted image 20230220144627.png|500]]

### 强化学习随机性
动作随机性：根据π函数确定
状态转移随机性：根据状态转移函数p随机抽样

## 回报（未来累计奖励）
R跟S和A有关
Rt和Rt+1的重要性不同，因此需要不同的权重，所以有折扣函数r

### 折扣回报
![[Pasted image 20230220145451.png|400]]
折扣函数需要自己调，对结果有很大影响

## 动作价值函数
![[Pasted image 20230220150223.png|360]]
R跟S、A有关，所有Ut与未来所有的S、A有关
因此无法知道在t时，U的值，因为未来对其有影响。
如何衡量当前状态的价值：求此状态和动作下的局部平均回报（把未来的参数积分积掉）。

> 条件期望概念（不严谨）：
> >E(X)是全体加权平均。
> >E(X|Y=y)是局部的加权平均，按照Y的不同取值，整个样本空间Ω被划分为n个互不相容的事件。
> >E(X|Y)是随机变量Y的函数。

>条件期望计算：
> ![[Pasted image 20230220154416.png|550]]

根据期望计算公式，需要知st时A的概率密度函数即：
![[Pasted image 20230220154853.png|500]]
所以Ut与π函数有关

### 最优动作价值函数
![[Pasted image 20230220155609.png|280]]
取使值最大的π函数

## 状态价值函数
![[Pasted image 20230220155720.png|250]]
取平均值，忽略A，可以衡量当下的状态情况
![[Pasted image 20230220160300.png|700]]





