 Mdnotes File Name: [[Liang_2020_DeepLearningBasedWireless]]

# 批注
文章的结构安排如下。

在第二节中，我们讨论了传统无线资源分配优化方法的局限性，并激励深度学习来解决该问题。
在第三节中，我们举例说明如何利用深度学习更有效地解决资源优化问题。
在第四节中，详细讨论了基于深度RL的方法，这些方法保证了在更灵活有效的框架下处理资源分配问题的根本转变。
在第五节中，我们认识到并强调了几个值得进一步研究的开放性问题。最后在第六节做出结束语。


第二节 传统无线资源分配优化方法的局限性

A. 传统优化方法的局限性

1.  大多数无线资源分配的优化问题是**强非凸**的(连续功率控制)、**组合**(离散信道分配)或**混合整数非线性规划**( MINLP ) (联合功率控制和频谱分配)。
    
    在多个用户共享信道中的频谱管理问题是**非凸**、**NP - hard**的。目前还没有已知的算法能够以多项式时间复杂度将问题求解到最优。
    
2.  现有优化方法的另一个局限性是**需要精确的模型，**并且求解高度依赖于模型的精度。而无线通信环境本质上是不断变化的，由此带来的**模型参数不确定性**，如信道状态信息( CSI )精度，会影响优化解的性能。
3.  最后，随着无线网络变得更加复杂和通用，许多**新的服务需求并没有直接转化为通信所习惯的性能指标**。例如，将V2V链路的可靠性定义为在时间T内成功交付大小为B的数据包，那么该问题就变成了一个跨越整个T的决策问题。传统的基于优化的方法在**每个时间步将问题分解为孤立的资源分配决策，而不考虑长期效应**。

B. 深度学习辅助优化

深度学习允许多层计算模型学习具有多个抽象层次的数据表示。每层计算上一层输出的线性组合，然后通过激活函数引入非线性以提高其表达能力。

1.  在简单的形式下，可以利用深度学习来**学习优化问题的参数和解的对应关系**。比如深度神经网络。
2.  深度学习可以作为一个组件嵌入，以**加速行为良好的优化算法的某些步骤**。

C. 基于深度强化学习的资源分配

1.  深度强化学习：可以建模为**马尔可夫决策过程**( MDP )。
2.  Deep Q-Network (DQN) 深度Q网络
3.  **Policy Gradient and Actor Critic** 政策梯度与行动者批判

车联网中的资源分配问题。该问题可以通过设计一个与最终目标相关的奖励信号来解决，并且学习算法可以自动地找出问题的一个合适的解决方案。

第三节 深度学习辅助的资源分配优化

1 )监督学习范式：利用DNNs学习从参数输入到给定优化算法解的映射。

1.  DNNs在降低计算复杂度的同时，在模仿最先进的最优或启发式算法的解决方案方面显示了它们的能力。
2.  利用通信网络特征提高样本效率。

缺点：1.受限于传统算法 2.需要大量样本

2 )目标导向的无监督学习范式：将优化目标作为损失函数，在训练时直接进行优化。

一般情况下，深度学习通过随机梯度下降来训练优化损失函数。损失函数从个案到个案进行设计。

3 )学习加速优化范式：将深度学习技术作为组件嵌入到给定优化算法的某些步骤中进行加速。

过程：

1.  训练阶段，通过使用模拟数据运行所涉及的优化问题的一些算法来生成标记样本。然后利用得到的训练数据集，通过更新DNN权重来最小化DNN输出与优化解之间的差异。
2.  测试阶段，我们同样使用相同的数学算法生成带标签的测试样本。我们将一个新问题实例的参数作为输入传递给训练好的网络，并收集推断的解，然后将其与对应的真实标签进行比较。