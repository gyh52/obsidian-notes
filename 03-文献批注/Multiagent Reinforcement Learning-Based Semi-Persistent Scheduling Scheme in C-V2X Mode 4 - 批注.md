 Mdnotes File Name: [[Gu_2022_MultiagentReinforcement]]

# 批注
本文提出了一种基于多智能体深度强化学习的SPS ( RL-SPS )算法，以减少数据包碰撞为目标，帮助车辆选择合适的无线资源。此外，采用多传感头注意力机制，通过帮助车辆选择性地关注邻居车辆的观察和动作来提高训练效率。

## 研究背景
C-V2X引入了两种无线电资源分配模式: mode3，mode4。在mode3中，无线资源由LTE基站通过Uu接口进行集中分配和调度。在mode4中，车辆通过PC5接口，根据SPS方案自主选择无线电资源。本文主要研究C-V2X模式下基于SPS方案的V2V通信。

SPS的性能对于实现在相邻车辆之间广播车辆安全信息的可靠直接通信至关重要。基于网络模拟器的SPS的性能表明，适当调整MAC层和PHY层的SPS的参数可以提供更好的性能。为了进一步了解SPS的可靠性，建立数据包接收比(PRR)的分析模型。

## 现有问题
SPS存在问题：
1.  存在隐藏终端的问题，传感结果可能不精确。
2.  半双工约束，车辆在传输时无法检测到其他车辆使用相同资源的存在，导致连续的包碰撞。
3.  在高密度场景下，相邻的车辆有重叠的可用资源列表，很可能同时选择相同的资源，从而增加包碰撞的概率。

## 已有解决办法
1.  优化参数配置
    1.  根据测量出的干扰水平来调整资源预留间隔和传输功率
    2.  自适应SPS方案，在不同车辆交通场景下调整资源预留间隔
2.  增强资源感知过程
    1.  给最近的干扰测量分配更高的权重来计算每个子帧的干扰水平
    2.  广播附加信息以提高资源感知的准确性
    3.  每次传输中与其他车辆共享重复计数器(reselection counter，RC)和子帧偏移信息
3.  改进资源分配过程
    1. 设置计数器来确定重新选择资源的时间，将包冲突的最大持续时间限制为一个可容忍的值。
    2. 分布式方法替代随机选择算法
4. 强化学习(RL)算法
    1.  完全分布式RL（distributed RL）
    2.  集中式RL（centralized RL）

## 创新点
提出了一种基于多智能体深度强化学习的SPS (a multiagent deep reinforcement learning-based SPS&nbsp;，RL-SPS)算法，帮助车辆选择合适的无线电资源，以减少分组碰撞。采用多头注意机制，帮助车辆选择性地注意邻近车辆的观察和动作，提高训练效率。

基于感知的半持续资源调度：

C-V2X设计了两个子通道: 物理侧链共享通道(PSSCH)传输传输块TB (transport block)和物理侧链控制通道(PSCCH)用于传输侧链控制信息(sidelink control information)消息。

在模式4中，车辆为固定传输频率的定期广播预留频道资源。根据SPS，每辆车用一个RC来保留所选的资源。RC在每次传输后减少1。当RC减为0时，资源重选过程以概率(1 - p)进行。

![[Pasted image 20230302170848.png|650]]

在模式4中，车辆为具有固定传输频率的定期广播保留信道资源。

每次传输后，RC减1。当RC减至0时，资源重新选择过程以概率（1-p）进行。p是重复使用相同资源的概率，p∈[0, 0.8]（接近0.8最好）。

车辆决定选择新的资源，筛选出选择窗口内的可用候选单子帧资源（CSR）:
1.  根据感知窗口（即前1000个子帧）中收到的SCI信息，排除被其他车辆占用的CSR。由于半双工限制，车辆无法感知感知窗口中使用的CSR。
2.  在感知窗口中连续测量资源的平均参考信号接收功率（RSRP）和接收信号强度指标（RSSI）。然后，排除平均RSRP高于阈值的未占用的CSR。至此，剩余的CSR构成资源列表List1。3GPP规定，List1的大小应至少达到选择窗口大小（总CSR的数量）的20%。否则，第2步迭代执行，RSRP阈值增量为3dB，直到实现这一目标。
3. 车辆创建一个List2，包括L1中RSSI较低的CSR，L2的大小被设定为选择窗口大小（总CSR的数量）的20%。然后，车辆从L2中随机选择一个CSR，并为下次传输保留一个新的RC。

RSSI(Received Signal Strength Indication)接收的信号强度指示：无线发送层的可选部分，用来判定链接质量，以及是否增大广播发送强度。