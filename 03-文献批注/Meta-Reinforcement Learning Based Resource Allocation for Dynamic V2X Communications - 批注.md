 Mdnotes File Name: [[Yuan_2021_MetaReinforcementLearning]]

# 现有算法的问题：
动态车辆环境和连续功率的量化

# 工作
1.  提出了一种基于深度强化学习( DRL )的资源分配算法来提高V2I和V2V链路的性能。该算法利用深度Q网络( DQN )解决子带分配问题，利用深度确定性策略梯度( DDPG )解决连续功率分配问题。
2.  提出了一种基于元的DRL算法来增强动态环境下资源分配策略的快速适应性。

# 实验
数值结果表明，与量化连续功率的DQN算法相比，所提出的基于DRL的算法能够显著提高性能。此外，所提出的基于元的DRL算法能够在经验有限的新环境中实现所需的快速适应。

# V2X通信的基本DRL架构图
![[Pasted image 20230514163435.png|600]]
  
<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F10122808%2Fitems%2FGZWJSBAF%22%5D%2C%22locator%22%3A%228967%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Yuan 等, 2021, p. 8967</span>)</span>

# 元强化学习
在MAML算法训练参数初始化的基础上，采用两级训练机制设计元强化学习算法：
1.  个体级更新：根据采样的小批量经验，学习每个任务的子带和功率选择策略。
2.  全局级更新：估计其对应查询集合Dval j上的损失函数，评估更新后的策略对每个任务的适应能力。聚合适应能力，对全局网络参更新。
过程：先个体级更新，再全局级更新。每个任务在继承全局共享初始化参数的基础上，对自身参数进行个体级更新，然后根据自身参数参与全局参数更新。