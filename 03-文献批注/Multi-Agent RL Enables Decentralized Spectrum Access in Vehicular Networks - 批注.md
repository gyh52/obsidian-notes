 Mdnotes File Name: [[Xiang_2021_MultiAgentRL]]

## 总结
1.  基于竞争循环双 Q 网络（D3RQN）的多智能体方法解决 V2V 信道频谱接入和发射功率的联合控制问题
2.  采用滞后 Q 学习与并发经验缓存轨迹（CERT）技术缓解 Non-stationary 问题。
3.  该方法要求节点之间交互经验数据，会对有限的网络资源产生较大的负荷。

## 工作
1.  提出了一种新的基于多智能体强化学习( Multi-Agent Reinforced Learning，MARL )的不依赖于信道状态信息( Channel State Information，CSI )的去中心化算法，在满足V2V链路时延和可靠性要求的同时，最大化V2I链路的总吞吐量。
2.  实现了个体双重对决深度循环Q网络( D3RQN )和精心设计的共同奖励来训练隐式协作代理，每个代理仅基于本地CSI无关的观测值单独优化策略。为了处理多智能体并发学习引起的非平稳性，我们结合滞后Q学习和并发经验回放轨迹( CERT )来稳定训练过程。此外，我们加入了近似后悔奖赏( Approximate后悔奖赏，ARR )来缓解环境动态性变化导致的奖赏估计不稳定问题。
3.  仿真结果表明，与集中式Brute - force方法相比，所提出的算法优于基准算法，并能获得接近的性能。此外，所提出的与CSI无关的设计表现出与CSI相关版本相当的性能，这为进一步降低基于机器学习的车载通信系统的信令开销提供了一些启示。